{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources: Acquiring Streaming Data\n",
    "\n",
    "Call to spark.readStream creates a `DataStreamReader` instance. This instance is in charge of managing the different stream formats and configurations provided through the builder method calls. \n",
    "\n",
    "If `DataStreamReader` was called *.format(...)* method, only after calling *.load(...)* method on `DataStreamReader` instance, the options provided to the builder are validated and, if everything checks out, streaming DataFrame is returned.\n",
    "\n",
    "If `DataStreamReader` was called with diffrent \"loading\" methods, such as *.json(...)*, *.csv(...)* and more, streaming DataFrame is returned immediately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available streaming source\n",
    "\n",
    "1. **File-based source** <br>\n",
    "    Monitors a path in a filesystem and consumes files atomically placed in it. The found files will then be parsed by the specified formatter and processed in order of file modification time.\n",
    "    Supported formats are: \n",
    "        * text\n",
    "        * csv\n",
    "        * json\n",
    "        * orc\n",
    "\n",
    "\n",
    "\n",
    "2. **Socket source**  <br>\n",
    "Establishes a client connection to a TCP server amd reads UTF-8 text data through a socket connection.\n",
    "3. **Kafka source** <br>\n",
    "Creates Kafka consumer able to retrieve data from Kafka.\n",
    "4. **Rate source & Rate per Micro-batch source** <br>\n",
    "Generates a stream of specified number of rows per second / specified number of rows per micro-batch . Each output row contains a *timestamp* and *value*.  Itâ€™s mainly intended as a testing source.\n",
    "5. **Table source** <br>\n",
    "Creates a Streaming DataFrame on a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/20 18:46:24 WARN Utils: Your hostname, DELEQ0283302041 resolves to a loopback address: 127.0.1.1; using 172.31.227.62 instead (on interface eth0)\n",
      "24/04/20 18:46:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/20 18:46:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/20 18:46:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/20 18:46:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"SourcesSinks\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"./warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/20 18:46:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/04/20 18:46:42 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/20 18:46:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/20 18:46:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/20 18:46:42 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DecimalType\n",
    "# define schema\n",
    "real_estate_schema = StructType(\n",
    "    [StructField('UID', IntegerType()), \n",
    "    StructField('Location', StringType(), True), \n",
    "    StructField('Price', DecimalType(15,2), True), \n",
    "    StructField('Bedrooms', IntegerType(), True), \n",
    "    StructField('Bathrooms', IntegerType(), True), \n",
    "    StructField('Size', IntegerType(), True), \n",
    "    StructField('Price SQ Ft', DecimalType(10,2), True), \n",
    "    StructField('Status', StringType(), True)])\n",
    "\n",
    "# Cleanup\n",
    "spark.sql(\"DROP TABLE IF EXISTS real_estate\")\n",
    "\n",
    "# create table from DataFrame\n",
    "real_estate_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .schema(real_estate_schema)\n",
    "    .csv(\"../data/batch_resource\", header = True))\n",
    "\n",
    "real_estate_df.write.saveAsTable('real_estate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"table\")\n",
    "    .table(\"real_estate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming DataFrame, like normal DataFrame, is lazily evaluated. What we get is a representation of the stream that we can use to express the series of transformations we want to apply.\n",
    "Creating a streaming DataFrame does not result in any data actually being consumed or processed until the stream is materialized. (like creating `Streaming Query` instance and calling *.start()* method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tranformation\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinks: Output the resulting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/20 18:50:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ce0080eb-6f5f-471a-8756-ebcf67d80d6c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/20 18:50:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "|   UID|          Location|    Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "|132842|     Arroyo Grande|795000.00|       3|        3|2371|     335.30|Short Sale|\n",
      "|134364|       Paso Robles|399000.00|       4|        3|2818|     141.59|Short Sale|\n",
      "|135141|       Paso Robles|545000.00|       4|        3|3032|     179.75|Short Sale|\n",
      "|135712|         Morro Bay|909000.00|       4|        4|3540|     256.78|Short Sale|\n",
      "|136282|Santa Maria-Orcutt|109900.00|       3|        1|1249|      87.99|Short Sale|\n",
      "|136431|            Oceano|324900.00|       3|        3|1800|     180.50|Short Sale|\n",
      "|137036|Santa Maria-Orcutt|192900.00|       4|        2|1603|     120.34|Short Sale|\n",
      "|137090|Santa Maria-Orcutt|215000.00|       3|        2|1450|     148.28|Short Sale|\n",
      "|137159|         Morro Bay|999000.00|       4|        3|3360|     297.32|Short Sale|\n",
      "|137570|        Atascadero|319000.00|       3|        2|1323|     241.12|Short Sale|\n",
      "|138053|Santa Maria-Orcutt|350000.00|       3|        2|1750|     200.00|Short Sale|\n",
      "|138730|Santa Maria-Orcutt|249000.00|       3|        2|1400|     177.86|Short Sale|\n",
      "|139291|     Arroyo Grande|299000.00|       2|        2|1257|     237.87|Short Sale|\n",
      "|139427|Santa Maria-Orcutt|235900.00|       3|        2|1400|     168.50|Short Sale|\n",
      "|139461|Santa Maria-Orcutt|348000.00|       3|        2|1600|     217.50|Short Sale|\n",
      "|139661|       Paso Robles|314000.00|       4|        3|1794|     175.03|Short Sale|\n",
      "|139918|        Los Alamos|399000.00|       4|        2|1850|     215.68|Short Sale|\n",
      "|139932|        San Miguel|599000.00|       3|        3|2950|     203.05|Short Sale|\n",
      "|140044|       Paso Robles|299000.00|       3|        2|1719|     173.94|Short Sale|\n",
      "|140073|   San Luis Obispo|425000.00|       3|        3|1472|     288.72|Short Sale|\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = real_estate_stream.writeStream.format(\"console\").queryName(\"Table stream\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query object we have just created is `StreamingQuery` instance. It provides a handle to query that is executing continuously in the background as new data arrives\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
