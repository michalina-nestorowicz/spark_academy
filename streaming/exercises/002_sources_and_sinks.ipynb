{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources: Acquiring Streaming Data\n",
    "\n",
    "Call to spark.readStream creates a `DataStreamReader` instance. This instance is in charge of managing the different stream formats and configurations provided through the builder method calls. \n",
    "\n",
    "If `DataStreamReader` was called with *.format(...)* method, only after calling *.load(...)* method on `DataStreamReader` instance, the options provided to the builder are validated and, if everything checks out, streaming DataFrame is returned.\n",
    "\n",
    "If `DataStreamReader` was called with diffrent \"loading\" methods, such as *.json(...)*, *.csv(...)* and more, streaming DataFrame is returned immediately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available streaming source\n",
    "\n",
    "1. **File-based source** <br>\n",
    "    Monitors a path in a filesystem and consumes files atomically placed in it. The found files will then be parsed by the specified formatter and processed in order of file modification time.\n",
    "    Supported formats are: \n",
    "        * text\n",
    "        * csv\n",
    "        * json\n",
    "        * orc\n",
    "\n",
    "\n",
    "\n",
    "2. **Socket source**  <br>\n",
    "Establishes a client connection to a TCP server and reads UTF-8 text data through a socket connection.\n",
    "3. **Kafka source** <br>\n",
    "Creates Kafka consumer able to retrieve data from Kafka.\n",
    "4. **Rate source & Rate per Micro-batch source** <br>\n",
    "Generates a stream of specified number of rows per second / specified number of rows per micro-batch . Each output row contains a *timestamp* and *value*.  It’s mainly intended as a testing source.\n",
    "5. **Table source** <br>\n",
    "Creates a Streaming DataFrame on a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/23 06:59:38 WARN Utils: Your hostname, DELEQ0283302041 resolves to a loopback address: 127.0.1.1; using 172.31.227.62 instead (on interface eth0)\n",
      "24/04/23 06:59:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/23 06:59:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"SourcesSinks\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"./warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 06:59:50 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/23 06:59:50 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/23 06:59:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/04/23 06:59:51 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore mnestoro@127.0.1.1\n",
      "24/04/23 06:59:51 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "24/04/23 06:59:53 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/04/23 06:59:53 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/23 06:59:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/23 06:59:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DecimalType\n",
    "# define schema\n",
    "real_estate_schema = StructType(\n",
    "    [StructField('UID', IntegerType()), \n",
    "    StructField('Location', StringType(), True), \n",
    "    StructField('Price', DecimalType(15,2), True), \n",
    "    StructField('Bedrooms', IntegerType(), True), \n",
    "    StructField('Bathrooms', IntegerType(), True), \n",
    "    StructField('Size', IntegerType(), True), \n",
    "    StructField('Price SQ Ft', DecimalType(10,2), True), \n",
    "    StructField('Status', StringType(), True)])\n",
    "\n",
    "# Cleanup workspace from previous runs\n",
    "spark.sql(\"DROP TABLE IF EXISTS real_estate\")\n",
    "\n",
    "# create table from DataFrame\n",
    "real_estate_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .schema(real_estate_schema)\n",
    "    .csv(\"../data/batch_resource\", header = True))\n",
    "\n",
    "real_estate_df.write.saveAsTable('real_estate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"table\")\n",
    "    .table(\"real_estate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming DataFrame, like normal DataFrame, is lazily evaluated. What we get is a representation of the stream that we can use to express the series of transformations we want to apply.\n",
    "Creating a streaming DataFrame does not result in any data actually being consumed or processed until the stream is materialized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinks: Output the resulting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We materialize a stream by calling `writeStream` on a streaming DataFrame. We need to define *where* and *how* we want the output data to go:\n",
    "<br>\n",
    "    **Where**: streaming sink <br>\n",
    "    **How**: output mode\n",
    "\n",
    "\n",
    "\n",
    "Calling `writeStream` on a streaming Dataset creates a `DataStreamWriter`. This is a builder instance that provides methods to configure the output behavior of our streaming process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available streaming sinks\n",
    "\n",
    "1. **Console sink**: A sink prints to the standard output\n",
    "2. **File sink**: File-based and format-specific sink that writes the results to a filesystem. \n",
    "3. **Kafka sink**: A Kafka-specific producer sink that is able to write to one or more Kafka topics.\n",
    "4. **Memory sink**: Creates an in-memory table using the provided query name as table name. This table receives continuous updates with the results of the stream.\n",
    "5. **Foreach sink**: Provides a programmatic interface to access the stream contents, one element at the time. Allows to run arbitrary computation on every row record in the output.\n",
    "6. **Foreach batch sink**: Provides a programmatic sink interface to the complete DataFrame that corresponds to each underlying microbatch of the Structured Streaming execution. Allows to run arbitrary operations and custom logic on the output of each micro-batch. \n",
    "7. **Table sink**: Starts the execution of the streaming query, which will continually output results to the given table as new data arrives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outputMode\n",
    "\n",
    "**`append`**(default mode) <br>\n",
    "Adds only final records to the output stream. A record is considered final when no new records of the incoming stream can modify its value. This is always the case with linear transformations like those resulting from applying projection, filtering, and mapping. This mode guarantees that each row will output only once.\n",
    "\n",
    "**`update`** <br>\n",
    "Adds new and updated records since the last trigger to the output stream. Update is meaningful only in the context of an aggregation, where aggregated values change as new records arrive. If more than one incoming record changes a single result, all changes between trigger intervals are collated into one output record.\n",
    "\n",
    "**`complete`** <br>\n",
    "Complete mode outputs the complete internal representation of the stream. This mode also relates to aggregations, because for nonaggregated streams, we would need to remember all records seen so far, which is unrealistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers\n",
    "`trigger` option lets us specify the frequency at which we want the results to be produced. By default, Structured Streaming will process the input and produce a result as soon as possible. When a trigger is specified, output will be produced at each trigger interval.\n",
    "\n",
    "* **Fixed interval micro-batches**: Query will be executed with user specified time interval that will dictate the frequency of the query results. Define `processingTime` interval parameter in string format, like '5 seconds'.\n",
    "* **Available**: Query will process all the available data and then stop on its own.\n",
    "* **Continuous**: Switches the execution engine to the experimental continuous engine for low-latency processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "`checkpointLocation` option points to the path where checkpoint data will be reliably stored.\n",
    "\n",
    "##### Checkpoint caveats\n",
    " \n",
    "The checkpoint operation poses additional requirements to the streaming application concerning the storage required to maintain the checkpoint data and the impact that this recurrent operation has on the performance of the application.\n",
    "\n",
    "![sawtooth_checkpoint_performance](pictures/checkpoint_sawtooth.png)\n",
    "\n",
    "Checkpoint “sawtooth” performance pattern [O'Reilly\"Stream Processing with Apache Spark\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Action\n",
    "`start()` materializes the complete job description into a streaming computation and initiates the internal scheduling process that results in data being consumed from the source, processed, and produced to the sink. `start()` returns a *`StreamingQuery`* instance, which provides a handle that is executing continuosly in the background as new data arrivews. Handle manages the individual life cycle of each query. This means that we can simultaneously start and stop multiple queries independently of one other within the same sparkSession.\n",
    "\n",
    "\n",
    "After this long introduction, let's finaly write output the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 07:00:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "|   UID|          Location|    Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "|132842|     Arroyo Grande|795000.00|       3|        3|2371|     335.30|Short Sale|\n",
      "|134364|       Paso Robles|399000.00|       4|        3|2818|     141.59|Short Sale|\n",
      "|135141|       Paso Robles|545000.00|       4|        3|3032|     179.75|Short Sale|\n",
      "|135712|         Morro Bay|909000.00|       4|        4|3540|     256.78|Short Sale|\n",
      "|136282|Santa Maria-Orcutt|109900.00|       3|        1|1249|      87.99|Short Sale|\n",
      "|136431|            Oceano|324900.00|       3|        3|1800|     180.50|Short Sale|\n",
      "|137036|Santa Maria-Orcutt|192900.00|       4|        2|1603|     120.34|Short Sale|\n",
      "|137090|Santa Maria-Orcutt|215000.00|       3|        2|1450|     148.28|Short Sale|\n",
      "|137159|         Morro Bay|999000.00|       4|        3|3360|     297.32|Short Sale|\n",
      "|137570|        Atascadero|319000.00|       3|        2|1323|     241.12|Short Sale|\n",
      "|138053|Santa Maria-Orcutt|350000.00|       3|        2|1750|     200.00|Short Sale|\n",
      "|138730|Santa Maria-Orcutt|249000.00|       3|        2|1400|     177.86|Short Sale|\n",
      "|139291|     Arroyo Grande|299000.00|       2|        2|1257|     237.87|Short Sale|\n",
      "|139427|Santa Maria-Orcutt|235900.00|       3|        2|1400|     168.50|Short Sale|\n",
      "|139461|Santa Maria-Orcutt|348000.00|       3|        2|1600|     217.50|Short Sale|\n",
      "|139661|       Paso Robles|314000.00|       4|        3|1794|     175.03|Short Sale|\n",
      "|139918|        Los Alamos|399000.00|       4|        2|1850|     215.68|Short Sale|\n",
      "|139932|        San Miguel|599000.00|       3|        3|2950|     203.05|Short Sale|\n",
      "|140044|       Paso Robles|299000.00|       3|        2|1719|     173.94|Short Sale|\n",
      "|140073|   San Luis Obispo|425000.00|       3|        3|1472|     288.72|Short Sale|\n",
      "+------+------------------+---------+--------+---------+----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = real_estate_stream.writeStream.format(\"console\").queryName(\"table_stream\").outputMode(\"append\").option(\"checkpointLocation\", \"../002checkpoint\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+--------+---------+--------+---------+----+-----------+----------+\n",
      "|   UID|Location|    Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|\n",
      "+------+--------+---------+--------+---------+----+-----------+----------+\n",
      "|999999| Wroclaw|729000.00|       3|        2|1844|     395.34|Short Sale|\n",
      "+------+--------+---------+--------+---------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new data to a table and investigate checkpoint location\n",
    "spark.sql(\"insert into real_estate VALUES (999999,'Wroclaw',729000.00,3,2,1844,395.34,'Short Sale')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor and manage the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e9fb6ce5-9bf7-4087-9cc0-4eff08c454df',\n",
       " 'runId': '653960dd-8e21-41d1-a4a1-bcc88e40e5aa',\n",
       " 'name': 'table_stream',\n",
       " 'timestamp': '2024-04-23T05:02:15.002Z',\n",
       " 'batchId': 2,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 0, 'triggerExecution': 0},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/home/mnestoro/SPARK/streaming/exercises/warehouse/real_estate]',\n",
       "   'startOffset': {'logOffset': 1},\n",
       "   'endOffset': {'logOffset': 1},\n",
       "   'latestOffset': None,\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6b5f4fef',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.lastProgress # the most recent progress update of this streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.awaitTermination() # # block until query is terminated, with stop() or with error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]], org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$3325/0x00007f45e51f9ea8@5c3bc2bc\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.default.real_estate[UID#163,Location#164,Price#165,Bedrooms#166,Bathrooms#167,Size#168,Price SQ Ft#169,Status#170] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/mnestoro/SPARK/streaming/exercises/warehouse/real_estate/pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<UID:int,Location:string,Price:decimal(15,2),Bedrooms:int,Bathrooms:int,Size:int,Price SQ F...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain() # Prints the physical plan to the console for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.exception()  # Returns the StreamingQueryException if the query was terminated by an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99cc2556-5438-41d8-b3f1-5f987714af92'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.runId  # get the unique id of this run of the query, which will be generated at every start/restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e9fb6ce5-9bf7-4087-9cc0-4eff08c454df'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.id # get unique id, persistant across restarts from checkpoint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop() # stop the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
