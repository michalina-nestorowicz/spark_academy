{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The company `SparkyCO` have noticed more than usual servers connectivity disruptions. You were given a task to investigate that and monitor all requests processed by the severs.\n",
    "\n",
    "You need to process sever access logs, monitor incoming traffic and detect any anomaly\n",
    "\n",
    "Logs schema:\n",
    "`host timestamp method endpoint protocol status content_size`\n",
    "\n",
    "1. Read incoming stream data using\n",
    "2. Parse incoming log (use provided regex expressions or any other method)\n",
    "3. Apply tumbling window aggregation of `15 minutes` and add 5 minut offset\n",
    "4. Filter \"localhost\" endpoints\n",
    "5. Order by \"count\"\n",
    "5. add watermark\n",
    "6. add boolean column \"possible_threat\", if `count` is over 1_000\n",
    "6. add checkpoint\n",
    "7. Write output to a file output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Log Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "For out practice purposes, we will generate mockup Apache Access logs.\n",
    ">1. Open terminal and change directory to `data` folder\n",
    ">2. In terminal, run command `python log_generator apache_access_schema.yaml`, add flag `-t` to truncate previously created logs\n",
    ">3. Based on provide .yaml file, new `log.txt.{id}` file should appear every 5s with 50 lines of random logs in `log/apache_access` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex patterns to parse logs\n",
    "\n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "time_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2}:\\d{4})]'\n",
    "general_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "content_size_pattern = r'\\s(\\d+)$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create DataFrame representing the stream of raw log data lines ariving to `/log/apache_access` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_lines = spark.readStream.text(\"../data/log/apache_access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parse log data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# logsDF = access_lines.select(regexp_extract('value', hostExp, 1).alias('host'),\n",
    "#                         regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
    "#                         regexp_extract('value', generalExp, 1).alias('method'),\n",
    "#                         regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "#                         regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "#                         regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
    "#                         regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Count every access status code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statusCountsDF=logsDF.groupBy(\"status\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Kick off our streaming query, dumping results to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = statusCountsDF.writeStream.outputMode(\"complete\").format(\"console\").queryName(\"status_counts\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Classroom cleanup\n",
    "IMPORTANT: Kill running log_generator script in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
